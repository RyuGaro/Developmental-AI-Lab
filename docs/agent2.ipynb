{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74eb8609-bd0c-4a26-94be-5cf56d9f58cb",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OlivierGeorgeon/Developmental-AI-Lab/blob/master/docs/agent2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# THE AGENT WHO THRIVED ON GOOD VIBES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bd8cd-4c59-427b-b0cb-84d3de0c14bc",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52effea-74dd-43c7-989a-7075634d3ccc",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement agents driven by a type of intrinsic motivation called 'interactional motivation.' This refers to the drive to engage in sensorimotor interactions that have a positive valence while avoiding those that have a negative valence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, _valences):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self._valences = _valences\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {_outcome}, \" \n",
    "                  f\"Prediction: {self._predicted_outcome == _outcome}, Valence: {self._valences[self._action][_outcome]}\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = 0\n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self._predicted_outcome = 0\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1c38f-c8ed-48e5-bc6a-e7078ed3e5e4",
   "metadata": {},
   "source": [
    "## Environment1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d65ca61-9386-4260-a406-ec766063569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \"\"\" In Environment 1, action 0 yields outcome 0, action 1 yields outcome 1 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        # return int(input(\"entre 0 1 ou 2\"))\n",
    "        if _action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6d7d6-5444-4778-b97a-cc5bd34c6cd2",
   "metadata": {},
   "source": [
    "## Environment2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91ae8fba-72aa-4194-be5a-2f27a1637e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    \"\"\" In Environment 2, action 0 yields outcome 1, action 1 yields outcome 0 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        if _action == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "## Define the valence of interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59da5f51-d5db-4cf4-8bd0-036ec11eaad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], \n",
    "            [1, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d25cea-a183-45c5-a624-1345aace245a",
   "metadata": {},
   "source": [
    "The valence table specifies the valence of each interaction. An interaction is a tuple (action, outcome):\n",
    "\n",
    "|| outcome 0 | outcome 1 |\n",
    "|---|---|---|\n",
    "| action 0 | -1 | 1 |\n",
    "| action 1 | 1 | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095ec84-3fcd-455c-bb3b-f9a72980048e",
   "metadata": {},
   "source": [
    "## Instantiate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "180453e4-128a-4f68-bb5f-2d4daf888d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent(valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16279c4-936b-4f37-a0c9-ca4b77609adf",
   "metadata": {},
   "source": [
    "## Instantiate the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17b3c8c8-0cbd-4f64-a97c-61519ad24dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce510-9438-47d1-ac88-ab79c063d592",
   "metadata": {},
   "source": [
    "## Test run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n"
     ]
    }
   ],
   "source": [
    "outcome = 0\n",
    "for i in range(10):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febad285-e8e5-4c24-a46e-e9519a27581c",
   "metadata": {},
   "source": [
    "Observe that, on each interaction cycle, the agent is mildly satisfied. On one hand, the agent made correct predictions, on the other hand, it experienced negative valence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9daa278-434e-43ce-8520-8fabef69cd83",
   "metadata": {},
   "source": [
    "Execute the agent in Environment2. Observed that it obtains a positive valence. \n",
    "\n",
    "Modify the valence table to give a positive valence when the agent selects action `0` and obtains outcome `0`.\n",
    "Observe that this agent obtains a positive valence in Environment1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199faee9-a546-4d86-8e09-484bf2212523",
   "metadata": {},
   "source": [
    "Implement Agent2 that selects actions that, it predicts, will result in an interaction that have a positive valence. \n",
    "\n",
    "Only when the agent gets bored does it select an action which it predicts to result in an interaction that have a negative valence. \n",
    "\n",
    "In the trace, you should see that the agent learns to obtain a positive valence during several interaction cycles.\n",
    "When the agent gest bored, it occasionnaly selects an action that may result in a negative valence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b9a28-121c-4379-8701-1d49ad197b8d",
   "metadata": {},
   "source": [
    "## Create Agent2 by overriding the class Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85b966d4-aca5-4a08-8413-d4906fdbbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2(Agent):\n",
    "    def __init__(self, _valences):\n",
    "        super().__init__(_valences)\n",
    "        self.action_outcome = {0: None, 1: None}\n",
    "        self.nb_corrects_in_a_row = 0\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._valences = _valences\n",
    "\n",
    "        \n",
    "    # TODO override the method action(self, _outcome)\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" On affiche les résultats précédents \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {_outcome}, \" \n",
    "                  f\"Prediction: {self._predicted_outcome == _outcome}, Valence: {self._valences[self._action][_outcome]}\")\n",
    "\n",
    "        \"\"\" On mémorise l'outcome de l'action précédente \"\"\"\n",
    "        self.action_outcome[self._action] = _outcome\n",
    "\n",
    "        # Si on ne sait pas quelle action choisir, on met 0 par défaut\n",
    "        if self._action is None:\n",
    "            self._action = 0 \n",
    "\n",
    "        \"\"\" Si la prédiction est correcte, on incrémente le compteur de corrects \"\"\"\n",
    "        if self._predicted_outcome == _outcome or self._valences[self._action][_outcome] > 0:\n",
    "            self.nb_corrects_in_a_row += 1\n",
    "        else:\n",
    "            self.nb_corrects_in_a_row = 0\n",
    "        \n",
    "        \"\"\" On choisit la prochaine action \"\"\"                                                                                                                                                        \n",
    "        # Si on a eu 4 corrects d'affilée, on change d'action\n",
    "        if self.nb_corrects_in_a_row == 4 or self._valences[self._action][_outcome] < 0:\n",
    "            # action opposée (action = 1 - action) [1 - 0 => 1, 1 - 1 => 0]\n",
    "            self._action = 1 - self._action \n",
    "            self.nb_corrects_in_a_row = 0\n",
    "\n",
    "        \"\"\" Selon l'action choisie, on prédit l'outcome si on peut \"\"\"\n",
    "        self._predicted_outcome = self.action_outcome[self._action]\n",
    "        # Si on ne sait pas, on met une valeur par défaut\n",
    "        if self._predicted_outcome is None:\n",
    "            self._predicted_outcome = 0\n",
    "        \n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b3f5c-f48c-45ad-bf1d-4a703bac64af",
   "metadata": {},
   "source": [
    "## Test your Agent2 in Environment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8025780-7d39-442d-9600-b40641e6d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Prediction: 0, Outcome: 1, Prediction: False, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n"
     ]
    }
   ],
   "source": [
    "a = Agent2(valences)\n",
    "e = Environment1()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02f401-3a78-4ced-9b8c-3b602c7be482",
   "metadata": {},
   "source": [
    "## Test your Agent2 in Environment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "925fb5f1-1eb6-4dee-a59b-fd6384e123d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 0, Outcome: 1, Prediction: False, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n"
     ]
    }
   ],
   "source": [
    "a = Agent2(valences)\n",
    "e = Environment2()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc8226-b4ac-49c7-b1a8-9b17adc19964",
   "metadata": {},
   "source": [
    "# Test your agent with a different valence table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a300f0-5c05-4bbf-a9a4-7ed17af0aae7",
   "metadata": {},
   "source": [
    "Note that, depending on the valence that you define, it may be impossible for the agent to obtain a positive valence in some environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49fdfd38-0d4e-48a3-ab0f-87177c2da97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 1, Prediction: False, Valence: -1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1\n"
     ]
    }
   ],
   "source": [
    "# Choose different valences\n",
    "valences = [[1, -1], \n",
    "            [1, -1]]\n",
    "# Run the agent\n",
    "a = Agent2(valences)\n",
    "e = Environment2()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15a134-34a6-4039-9b43-f8fd76d93b5e",
   "metadata": {},
   "source": [
    "## Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e661c15-c1ed-4512-a1d9-0fbdb8f91879",
   "metadata": {},
   "source": [
    "Explain what you programmed and what results you observed. Export this document as PDF including your code, the traces you obtained, and your explanations below (no more than a few paragraphs):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bbc0899-eee2-404a-baf6-47741067e7d9",
   "metadata": {},
   "source": [
    "Pour l'Agent2, nous avons développé un agent qui en plus de l'Agent1 prend en compte des interaction. Une interaction est un tuple (action, outcome) qui a une certaine valeur. Ici, les valeurs des interactions sont soit négative (-1), soit positive (1). \n",
    "L'agent2 essaie toujours de prédire la sortie de l'action qu'il a choisit mais regarde et prends en compte la valence l'interaction qui en découle.\n",
    "L'agent2 va préférer une valence positive à une valence négative. De ce fait, si il reçoit une valence négative il va changer d'action pour choisir une action qui lui donne un valence positive, tout en faisant attention que l'agent ne s'ennuie pas et que si il commence à s'ennuyer, alors il va changer d'action même si cela implique d'avoir une valence négative.\n",
    "\n",
    "Tableau de valence utilisé pour les 2 premières traces:\n",
    "|| outcome 0 | outcome 1 |\n",
    "|---|---|---|\n",
    "| action 0 | -1 | 1 |\n",
    "| action 1 | 1 | -1 |\n",
    "\n",
    "Dans l'image ci-dessous l'agent évoluant dans l'environnemen1 ne reçoit que des valences négatives. Cela est dû au tableau de valence qui fait que peut importe l'action que choisit l'agent il recevra toujours une valence négative. Par conséquent il va à chaque fois changer l'action\n",
    "\n",
    "<img src='img/Agent2_env1_tv1.png'  width=500px>\n",
    "\n",
    "A l'inverse, toujours à cause du tableau de valence choisit, dans l'environnemnt2, l'agent ne va recevoir que des valences positives ce qui implique qu'il ne va changer d'action uniquelent lorsqu'il va s'ennuyer\n",
    "\n",
    "<img src='img/Agent2_env2_tv1.png'  width=500px>\n",
    "\n",
    "Pour avoir quelque chose de moins binaire (recevoir que des valences positives ou que négatives), nous avons défini un nouveau tableau de valencesqui en fonction de l'action choisi revoit soit une valence positive soit une valence négative.\n",
    "\n",
    "Tableau de valence choisit: \n",
    "|| outcome 0 | outcome 1 |\n",
    "|---|---|---|\n",
    "| action 0 | 1 | -1 |\n",
    "| action 1 | 1 | -1 |\n",
    "\n",
    "Ci-dessous la trace de l'agent2 dans l'environnement2 avec le nouveau tableau de valence.\n",
    "Avec ce tableau de valence et dans cet environnement, lorque l'agent choisit l'action 1 il valence recevoir une valence positive et une valence négative pour l'action 0.\n",
    "L'agent va donc effectuer l'action 1 jusqu'à ce qu'il s'ennuie pouis l'action 0 une fois et revenir à l'action 1.\n",
    "\n",
    "<img src='img/Agent2_env2_tvp.png'  width=500px>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c27cb3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
